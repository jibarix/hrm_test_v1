# Complete laptop-optimized config for Simple Pathfinding
# Hardware: RTX 3070 Ti Laptop (8GB VRAM)  
# Dataset: 1000 examples, 6-token system
# WITH SPATIAL CONNECTIVITY FIX + HYBRID LOSS APPROACH

defaults:
  - _self_

hydra:
  output_subdir: null

# Data path (updated for simplified pathfinding)
data_path: data/simple-pathfinding-1k

# Architecture
arch:
  name: hrm.hrm_act_v1@HierarchicalReasoningModel_ACTV1
  
  # Loss configuration WITH SPATIAL FIX + HYBRID APPROACH
  loss:
    name: losses@ACTLossHead
    loss_type: stablemax_cross_entropy
    
    # SPATIAL CONNECTIVITY FIX - Core parameters
    enable_spatial_loss: false         # Enable the fundamental spatial fix
    spatial_penalty_weight: 10.0       # Penalty strength for impossible spatial jumps
    enable_connectivity_loss: false    # Start simple - disable additional connectivity check
    connectivity_weight: 5.0           # (unused when connectivity disabled)
    
    # HYBRID LOSS APPROACH - New parameters
    enable_focal_loss: false            # Use focal loss for class imbalance
    focal_alpha: 0.25                   # Recommended alpha value
    focal_gamma: 2.0                    # Recommended gamma value
    enable_dice_loss: false             # Add dice loss for path overlap
    dice_weight: 5.0                    # Weight for dice loss component
    
    # Grid and token configuration (simplified 6-token system)
    grid_width: 40                     # 40Ã—40 grid dimensions
    path_token_id: 6                   # PATH token in simplified system

  # ACT (Adaptive Computation Time) settings
  halt_exploration_prob: 0.1           # Standard exploration probability
  halt_max_steps: 8                    # Max reasoning steps

  # Hierarchical reasoning cycles
  H_cycles: 1                          # High-level planning cycles
  L_cycles: 2                          # Low-level execution cycles

  # Transformer architecture (laptop-optimized)
  H_layers: 2                          # High-level module layers
  L_layers: 2                          # Low-level module layers
  hidden_size: 64                      # Model dimension (reduced for 8GB VRAM)
  num_heads: 2                         # Attention heads (reduced for efficiency)
  expansion: 4                         # FFN expansion ratio

  # Embeddings
  puzzle_emb_ndim: 64                  # Same as hidden_size

  # Position encodings
  pos_encodings: rope                  # Rotary position embeddings
  rms_norm_eps: 1e-5                   # RMSNorm epsilon
  rope_theta: 10000.0                  # RoPE base frequency

  # Forward pass dtype
  forward_dtype: bfloat16              # Mixed precision training

# Training hyperparameters (laptop-optimized)
global_batch_size: 16                 # Conservative for 8GB VRAM
epochs: 200                           # Suitable for 1K dataset
eval_interval: 20                     # Frequent evaluation for debugging
checkpoint_every_eval: true           # Save checkpoints at each eval

# Learning rates
lr: 5e-5                              # Main learning rate
lr_min_ratio: 0.1                     # LR decay to 10% of original
lr_warmup_steps: 50                   # Short warmup for small dataset

# Optimizer settings (Adam-like, as in paper)
beta1: 0.9                            # Adam beta1
beta2: 0.95                           # Adam beta2 (slightly higher than default)

# Regularization
weight_decay: 0.1                     # Strong regularization for small dataset
puzzle_emb_weight_decay: 0.1          # Strong puzzle embedding regularization

# Puzzle embedding learning rate
puzzle_emb_lr: 5e-5                   # Same as main LR for simplicity

# Project and run naming (updated for simplified pathfinding)
project_name: null                    # Auto-generate: "Simple-pathfinding-1k ACT-torch"
run_name: null                        # Auto-generate: "HierarchicalReasoningModel_ACTV1 adjective-animal"
checkpoint_path: null                 # Auto-generate based on project/run names

# Experiment settings
seed: 0                               # Reproducibility
eval_save_outputs: ["inputs", "labels", "logits", "puzzle_identifiers"]  # Save for analysis

# ========================================================================
# HYBRID LOSS VALIDATION NOTES
# ========================================================================
# Key metrics to monitor for hybrid approach validation:
#
# SUCCESS INDICATORS:
# - train/focal_loss: Should decrease as model learns to focus on hard examples
# - train/dice_loss: Should decrease as path overlap improves
# - train/spatial_loss: Should decrease as spatial constraints are learned
# - train/exact_accuracy: Should increase without path_ratio=1.0 cheating
# - train/path_ratio: Should stay reasonable (<0.3), not spam entire grid
#
# FAILURE INDICATORS:
# - train/focal_loss: Stays high (not learning hard examples)
# - train/dice_loss: Stays high (poor path overlap)
# - train/exact_accuracy: Stays at 0% (fundamental learning failure)
# - train/path_ratio: Goes to 1.0 (reverting to cheating behavior)
#
# TUNING GUIDELINES:
# - If focal_loss dominates, reduce focal_gamma to 1.5
# - If dice_loss dominates, reduce dice_weight to 2.0
# - If spatial_loss dominates, reduce spatial_penalty_weight to 5.0
# - If training is unstable, disable one component at a time for debugging
#
# Expected training progression with hybrid approach:
# - Steps 0-200: All loss components high as model explores
# - Steps 200-800: Focal and dice losses decrease as learning begins
# - Steps 800-1200: Breakthrough - exact_accuracy jumps to >10%
# - Steps 1200+: Continued improvement with stable loss convergence
# ========================================================================