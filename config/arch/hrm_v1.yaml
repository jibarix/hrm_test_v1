name: hrm.hrm_act_v1@HierarchicalReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: stablemax_cross_entropy

halt_exploration_prob: 0.15
halt_max_steps: 16

H_cycles: 2
L_cycles: 2

H_layers: 4
L_layers: 4

hidden_size: 256               # Reduced: Appropriate for our hardware (was 512)
num_heads: 4                   # min(2, hidden_size // 64)
expansion: 4

puzzle_emb_ndim: ${.hidden_size}

pos_encodings: rope

# MODIFICATIONS EXPLAINED:
# 1. halt_exploration_prob: Sasme as paper
# 2. halt_max_steps: 8 - Smaller but consistent computation budget  
# 3. This gives Q-learning examples of "thinking" vs "immediate answers"